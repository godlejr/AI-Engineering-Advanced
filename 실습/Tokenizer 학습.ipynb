{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"TX3sJcWU5iYI"}},{"cell_type":"code","source":["!pip install Korpora"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzHWu2k7MfqC","executionInfo":{"status":"ok","timestamp":1729147077012,"user_tz":-540,"elapsed":3615,"user":{"displayName":"김동주","userId":"11969625730817466087"}},"outputId":"37cde56f-483d-4c0d-d9d4-0ec7f0f2b750"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from Korpora) (0.6)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.26.4)\n","Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.5)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.32.3)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2024.8.30)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-46z0xg-vDDE"},"outputs":[],"source":["from transformers import AutoTokenizer\n","from Korpora import Korpora\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"YTspp3hiMevU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"5VBOdTFKNARV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zrm2D14gvDDI","outputId":"fbde4b9f-5c6b-4001-a52d-1a6f566fa85a","colab":{"base_uri":"https://localhost:8080/","height":823},"executionInfo":{"status":"error","timestamp":1729147423452,"user_tz":-540,"elapsed":341023,"user":{"displayName":"김동주","userId":"11969625730817466087"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n","    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n","\n","    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n","    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n","    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n","\n","    # Description\n","    Author : Hyunjoong Kim lovit@github\n","    Repository : https://github.com/lovit/kowikitext\n","    References :\n","\n","    한국어 위키피디아의 덤프 데이터를 바탕을 제작한 wikitext 형식의 텍스트 파일입니다.\n","    학습 및 평가를 위하여 위키페이지 별로 train (99%), dev (0.5%), test (0.5%) 로 나뉘어져있습니다.\n","\n","\n","    # License\n","    CC-BY-SA 3.0 which kowiki dump dataset is licensed\n","\n"]},{"output_type":"stream","name":"stderr","text":["[kowikitext] download kowikitext_20200920.train.zip: 494MB [00:06, 70.8MB/s]                          \n"]},{"output_type":"stream","name":"stdout","text":["unzip /root/Korpora/kowikitext/kowikitext_20200920.train\n"]},{"output_type":"stream","name":"stderr","text":["[kowikitext] download kowikitext_20200920.test.zip: 2.51MB [00:00, 7.24MB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["unzip /root/Korpora/kowikitext/kowikitext_20200920.test\n"]},{"output_type":"stream","name":"stderr","text":["[kowikitext] download kowikitext_20200920.dev.zip: 2.37MB [00:00, 6.51MB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["unzip /root/Korpora/kowikitext/kowikitext_20200920.dev\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ce32f9c3b70d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Korpora 라이브러리를 사용하여 \"kowikitext\" 데이터셋을 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kowikitext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 모든 텍스트 데이터를 리스트에 저장합니다. 비어 있는 텍스트는 제외\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/Korpora/loader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, corpus_names, root_dir, force_download)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcorpus_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mcorpora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mKORPUS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorpus_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/Korpora/loader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mcorpus_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mcorpora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mKORPUS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorpus_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/Korpora/korpus_kowiki.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, force_download)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 response = input(\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0;34m'kowikiText.train text file is large (1.6G).\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0;34m'If you want to load text in your memory, please insert `yes`\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["# Korpora 라이브러리를 사용하여 \"kowikitext\" 데이터셋을 로드\n","corpus = Korpora.load(\"kowikitext\")\n","# 모든 텍스트 데이터를 리스트에 저장합니다. 비어 있는 텍스트는 제외\n","final_corpus = [i for i in corpus.get_all_texts() if i]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juavRGQsvDDJ"},"outputs":[],"source":["print(final_corpus[0:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m489Q8-7vDDK"},"outputs":[],"source":["#  리스트를 10,000개씩 나누어 배치를 생성: 학습시 데이터셋을 순차적으로 제공하는 역할\n","def batch_generation(final_corpus):\n","    for i in range(0, len(final_corpus), 10000):\n","        yield final_corpus[i : i + 10000]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IljcWhqtvDDL"},"outputs":[],"source":["# Tokenizer\n","tokenizer_dir = 'mistralai/Mistral-7B-v0.1'\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXtvptIgvDDM"},"outputs":[],"source":["# 한국어 데이터셋으로 새로운 tokenizer 학습하는 코드\n","# 토크나이저를 주어진 데이터셋으로 완전히 새로 학습하는 방식\n","new_tokenizer = tokenizer.train_new_from_iterator(batch_generation(final_corpus), 35000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRLiGKrkvDDM"},"outputs":[],"source":["# 한국어 데이터셋으로 학습된 BPE 토크나이저 저장\n","new_tokenizer.save_pretrained('./new_tokenizer')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZ1tVDrevDDN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufnsG_XSvDDN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbyEDq92vDDN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae0XfRi2vDDO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UooGvPbpvDDO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqaBV2b-vDDO"},"outputs":[],"source":["with open(\"./old_tokenizer/tokenizer.json\", \"r\") as json_file:\n","    l_info = json.load(json_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXRv0KF8vDDO"},"outputs":[],"source":["with open(\"./new_tokenizer/tokenizer.json\", \"r\") as json_file:\n","    n_info = json.load(json_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzwp5IjDvDDO"},"outputs":[],"source":["origin_vocabs_set = set(l_info['model']['vocab'])\n","expanded_vocab_dict = dict(l_info['model']['vocab'])\n","expanded_merge_list = list(l_info['model']['merges'])\n","expanded_vocab_pos = len(expanded_vocab_dict)\n","merge_rules = n_info['model']['merges']\n","\n","for token in tqdm(list(filter(lambda x: not x.endswith('\\n') and re.match('^[ㄱ-ㅎ가-힣▁]+$', x), n_info['model']['vocab'].keys()))):\n","    if token not in origin_vocabs_set and len(token) > 1:\n","        expanded_vocab_dict[token] = expanded_vocab_pos\n","        expanded_vocab_pos += 1\n","        expanded_merge_list.extend(list(filter(lambda x: x.replace(' ', '')==token, merge_rules)))\n","\n","for token in tqdm(list(filter(lambda x: not x.endswith('\\n') and re.match('^[ㄱ-ㅎ가-힣▁]+$', x), n_info['model']['vocab'].keys()))):\n","    if token not in origin_vocabs_set and len(token) == 1:\n","        expanded_vocab_dict[token] = expanded_vocab_pos\n","        expanded_vocab_pos += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkLGQ7J1vDDP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X72YlgwPvDDP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfWyp3xpvDDP"},"outputs":[],"source":["l=l_info['model']['vocab']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bc64RuWPvDDP"},"outputs":[],"source":["print(len(expanded_vocab_dict))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBcd-XwSvDDP"},"outputs":[],"source":["print(len(l)) # 32000\n","print(len(expanded_vocab_dict)) # 47383\n","\n","# 기존 토크나이저의 정보를 복사\n","e_info = copy.deepcopy(l_info)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NK-AEti6vDDP"},"outputs":[],"source":["\n","# vocab과 merges만 교\n","e_info['model']['vocab'] = expanded_vocab_dict\n","e_info['model']['merges'] = expanded_merge_list\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLJWsldRvDDP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hh3kcrZTvDDQ"},"outputs":[],"source":["# Write the modified data back to the file\n","with open(\"./old_tokenizer/tokenizer.json\", \"w\") as file:\n","    json.dump(e_info, file, indent=2, ensure_ascii = False)  # Using indent for pretty-printing\n","\n","e = AutoTokenizer.from_pretrained('./old_tokenizer/')\n","\n","print(len(e)) # 47383"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-1_lGi4vDDQ"},"outputs":[],"source":["e = AutoTokenizer.from_pretrained('./old_tokenizer/')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHZpALl5vDDQ"},"outputs":[],"source":["e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgGAbXOZvDDQ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"lee","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}